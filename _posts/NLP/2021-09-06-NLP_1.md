---
title:  "[부스트캠프 AI Tech / Day25] NLP - Intro to NLP, Bag-of-Words"

categories:
  - NLP
tags:
  - NLP
toc: true
toc_sticky: true
 
date: 2021-09-06
last_modified_at: 2021-09-07
---

# Intro to Natural Language Processing(NLP)  
<br>
NLP : NLU(Natural Language Understanding) + NLG(Natural Language Generation)  
    NLU : 컴퓨터가 주어진 단어나 문장, 문단 글을 이해하는  
    NLG : 자연어를 상황에 따라 적절히 생성할수 있는  
<br>
    
## Academic Disciplines realted to NLP  
* Natural langauge processing  
    * Low-level-parsing  
        * Tokenization : 문장을 단어단위로 쪼개는 것  
        * Stemming : 어미가 변해도 단어의 어근(의미) 보존  
    * Word and phrase level  
        * Named entity recognition(NER) : 단일단어, 여러 단어로 이루어진 고유명사를 인식하는 Task  
        * Part-of-Speech(POS) tagging : 문장내에서 단어들의 품사를 식별하여 태그를 붙여주는 것  
    * Sentence level  
        * Sentiment analysis : 긍 부정 예측  
        * machine translation : 번역(이해 및 문법)  
    * Multi-sentence and paragraph level  
        * Entailment prediction : 두 문장간의 논리적인 내포, 모순 관계 예측  
        * Question answering : 독해기반 질의응답, 문서로 부터 독해를 통해 질문에 답  
        * Dialog systems : chat-bot 대화 실행  
        * Summarization : 요약  
* Text mining : 빅데이터 분석과 관련(trend 분석, keyword 분석)  
    * Extract useful information and insights from text and document data  
        * analyzing the trends of keywords from massive news data  
    * Document clustering (topic modeling)  
        * clustering news data and grouping into different subjects  
    * Highly related to computational social science  
        * analyzing the evolution of people's political tendency based on media data  
* Information retrieval(정보검색, 검색기술)  
    * Highly related to computational social science  
        * not actively studied now  
        * evovled into a recommendation system  
## Trends of NLP  
* Word2Vec, GloVe : each word can be represented as a vector  
* RNN-family models : LSTM, GRUs  
* attention modules and Transformer models : replaced RNNs with self-attention  
* BERT, GPT-3 : huge models by stacking its basic module, self-attention, and these models are trained with large-sized datasets through language modeling tasks, one of the self-supervised training setting that does not require additional labels for a particular task  
<br>

# Bag-of-Words  
* Step 1 : Constructing the vocabulary containing unique words  
    * 중복단어 제거  
* Step 2 : Encoding unique words to one-hot vectors  
    * 각각의 word를 categorical variable(범주형 변수)  
    * 단어의 의미에 상관없이 모두 동일한 관계로 설정  
        * distance is <math> sqrt{2} </math>

