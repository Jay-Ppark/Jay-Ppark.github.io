---
title:  "[부스트캠프 AI Tech / Day8] DL Basic - Modern Convolutional Neural Networks"

categories:
	- Deep Learning Basics
tags:
	- DL Basic
toc: true
toc_sticky: true
 
date: 2021-08-11
last_modified_at: 2021-09-02
---

# Modern Convolutional Neural Networks  
<br>

## AlexNet  
* [image]  
* Key ideas  
	* ReLU activation  
		* Preserves properties of linear models  
		* Easy to optimize with gradient descent  
		* Good generalization  
		* Overcome the vanishing gradient problem  
	* GPU implementation (2 GPUs)  
	* Local response normalization, Overlapping pooling  
	* Data augumentation  
	* Dropout  
## VGGNet  
* Increasing depth with 3X3 convolutional filters(with stride 1)  
* 1X1 convolution for fully connected layers(이 방법으로 파라미터 수 줄이려 하지 않음)  
* Dropout (p=0.5)  
* VGG16, VGG19  
* Why 3 X 3 convolution?  
	* 크기가 커지면 receptive field가 커짐  
	* [그래프]  
## GoogLeNet  
* [그래프]  
* network in network 구조  
* Inception blocks  
	* Benefit of 1X1 convolution  
	* reduce the number of parameter  
	* 여러개로 퍼졌다가 1개로  
	* convolution filter 하기전 1X1이 들어감  
## ResNet  
* Deeper neural networks are hard to train  
	* Overfitting is usually caused by an excessive number of parameters  
* Add an identity map (skip connection)  
	* 학습을 잘 시킴 -> layer를 더 쌓을 수 있음  
	* Add an identity map after nonliner activations  
	* [그래프]  
* Batch normalization after convolutions  
	* [그래프]  
* Bottleneck architecture  
	* GoogLeNet Interception structure  
* Performance increases while parameter size decreases  
## DenseNet  
* DenseNet uses concatenation instead of addition  
	* [graph]  
* Dense Block  
	* Each layer concatenates the feature maps of all preceding layers  
	* The number of channels increases geometrically  
* Transition Block  
	* BatchNorm -> 1X1 Conv -> 2X2 AvgPooling  
	* Dimension reduction  
## Summary  
* VGG : repeated 3X3 blocks  
* GoogLeNet : 1X1 convolution  
* ResNet : skip-connection  
* DenseNet : concatenation  
