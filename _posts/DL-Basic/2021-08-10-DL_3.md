---
title:  "[부스트캠프 AI Tech / Day7] DL Basic - Optimization"

categories:
	- Deep Learning Basics
tags:
	- DL Basic
toc: true
toc_sticky: true
 
date: 2021-08-10
last_modified_at: 2021-09-02
---

# Optimization  
<br>

## Introduction  
* Gradient Descent  
	* First-order iterative optimization algorithm for finding a local minimum of a differentiable function  
## Important Concepts in Optimization  
### Generalization  
* Generalization gap  
	* Test error 와 Training error 차이  
	[그래프]
### Underfitting vs Overfitting  
* Underfitting : 학습 데이터가 없거나 네트워크가 너무 간단하거나 trainging을 조금 시킨 경우  
[그래프]  
* Overfitting : 학습 데이터는 잘동작하나 테스트 데이터에서 동작을 잘 못함  
[그래프]  
### Cross-validation  
* hyper parameter를 찾음  
* K-fold validation  
	* 학습 데이터를 K개로 나누고 K-1개로 학습하고, 1개로 test  
### Bias and Variance  
[그래프]  
* Bias and Variance Tradeoff  
[수식]  
### Bootstrapping  
* 학습 데이터가 고정되어 있을때 sub-sampling을 통해 학습데이터를 여러개 만들고 여러 모델을 만들어서 무엇인가 하는 것을 의미  
* Bootstrapping is any test or metric that uses random sampling with replacement  
### Bagging vs Boosting  
* Bagging(Bootstrapping aggregating)  
	* 1개 모델보다 성능이 더 좋을때가 많음  
	* ex) output을 평균, min-max값 제외하고 평균 등 다양한 방법  
* Boosting  
	* weak learner들을 sequential하게 합쳐서 1개의 strong learner를 만듬  
[그래프]  
<br>

## Practical Gradient Descent Methods  
* Gradient Descent Methods  
	* Stochastic gradient descent  
		* Update with the gradient computed from a single sample  
		* 여러개 중 한번에 1개만 gradient update 반복  
	* Mini-batch gradient descent  
		* Update with the gradient computed from a subset of data  
		* batch-size -> gradient 구하고 update  
	* Batch gradient descent  
		* Update with the computed from the whole data  
		전체를 다 사용해서 gradient 평균 update  
* Batch-size Matters  
	* batch-size가 중요  
	* large batch : sharp minimizers  
	* small batch : flat minimizers (flat이 더 좋음)  
[그래프]  
* Gradient Descent Methods  
	* Stochastic gradient descent  
		* [graph] [function]    
	* Momentum  
		* 관성 : 이쪽방향으로 가면 다음번도 이쪽으로 가려는 정보 이용  
		* [graph] [function]  
	* Nesterov accelerated gradient  
		* lookahead gradient : 한번이동 a라는 현재 정보로 그 방향으로 가보고 간곳에서 gradient 계산한것으로 accumulation 함  
		* [graph] [function]  
	* Adagrad  
		* 많이 변한 파라미터 : 적게 변화시킴  
		* 조금 변한 파라미터 : 많이 변화 시킴  
		* G : 각 파라미터가 얼마나 변했는지 제곱해서 더함  
			* 무한대로 커지면 update 거의 안됨 -> 학습이 멈춤  
		* [graph] [function]
	* Adadelta  
		* G가 계속해서 커지는 것을 막음  
		* no learning rate  
		* [graph] [function]   
	* RMSprop  
		* [graph] [function]  
	* Adam  
		* cobines momentum with adaptive learning rate  
## Regularization  
	* 일반화가 잘되게 하기 위해서, test가 잘되게  
* Early stopping  
	* validation error를 활용  
* Parameter norm penalty  
	* parameter 너무 커지지않게 하는 것  
	* 가정 : 부드러운 함수일수록 일반화가 높을 것이다  
	* 파라미터들을 다 제곱해서 더한 값을 줄이는 것  
* Data augementation  
	* More data are always welcomed  
	* [예시]  
* Noise robustness  
	* Add random noises inputs or weights  
* Label smoothing  
	* Mix-up : mixing both input and output of two randomly selected training data  
		* data를 2개를 뽑아서 섞는 것  
		* decision boundary를 부드럽게  
	* CutMix : mixing inputs with cut and paste and outputs with soft labels of two randomly training data  
* Dropout  
	* Each forward pass, randomly set some neurons to zero  
* Batch normalization  
	* compute the empirical mean and variance independently for each dimension (layers) and normalize  
	* 적용하고자하는 layer에 통계를 정규화시킴  
	* [예시]  
    